{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liberaries Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import face_recognition\n",
    "from deepface import DeepFace\n",
    "import json\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 10 face(s) in this photograph.\n"
     ]
    }
   ],
   "source": [
    "# Load an image from file\n",
    "image = face_recognition.load_image_file(\".\\Images\\group\\group_image.png\")\n",
    "\n",
    "# Detect all faces in the image\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "# Print the number of faces detected\n",
    "print(f\"Found {len(face_locations)} face(s) in this photograph.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image from RGB (used by face_recognition) to BGR (used by OpenCV)\n",
    "image_bgr = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# Draw a rectangle around each face\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    cv2.rectangle(image_bgr, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Faces Found', image_bgr)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the face encodings for each face\n",
    "face_encodings = face_recognition.face_encodings(image, face_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Iterate over the images in the Individuals folder\n",
    "for filename in os.listdir(\".\\images\\individuals\"):\n",
    "    if filename.endswith(\".jpeg\"):\n",
    "        known_image = face_recognition.load_image_file(f\".\\Images\\Individuals\\{filename}\")\n",
    "        known_encoding = face_recognition.face_encodings(known_image)[0]\n",
    "        known_face_encodings.append(known_encoding)\n",
    "        known_face_names.append(filename.replace(\".jpeg\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image with an unknown face\n",
    "unknown_image = face_recognition.load_image_file(\".\\Images\\group\\group_image.png\")\n",
    "\n",
    "# Find all face locations and face encodings in the unknown image\n",
    "unknown_face_locations = face_recognition.face_locations(unknown_image)\n",
    "unknown_face_encodings = face_recognition.face_encodings(unknown_image, unknown_face_locations)\n",
    "\n",
    "# Loop over each face found in the unknown image\n",
    "for face_encoding, face_location in zip(unknown_face_encodings, unknown_face_locations):\n",
    "    \n",
    "    # See if the face is a match for the known faces\n",
    "    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "    \n",
    "    name = \"Unknown\"\n",
    "\n",
    "    # If a match was found, use the known person's name\n",
    "    if True in matches:\n",
    "        first_match_index = matches.index(True)\n",
    "        name = known_face_names[first_match_index]\n",
    "\n",
    "    # Draw a box around the face\n",
    "    top, right, bottom, left = face_location\n",
    "    cv2.rectangle(unknown_image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "    # Label the image with the name\n",
    "    cv2.putText(unknown_image, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "\n",
    "\n",
    "# Display the result\n",
    "cv2.imshow('Face Recognition', unknown_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Static Face Attributes detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.12it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.28it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.23it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.66it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.80it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.88it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.91it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.90it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.87it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.92it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Abdallah_Mohamed': [{'age': 23,\n",
      "                       'dominant_emotion': 'happy',\n",
      "                       'dominant_gender': 'Man',\n",
      "                       'dominant_race': 'middle eastern',\n",
      "                       'emotion': {'angry': 6.378863684444887e-06,\n",
      "                                   'disgust': 5.045902219280377e-07,\n",
      "                                   'fear': 0.0025336362942147282,\n",
      "                                   'happy': 99.31600685811084,\n",
      "                                   'neutral': 0.6674011170060108,\n",
      "                                   'sad': 0.013002829702739699,\n",
      "                                   'surprise': 0.0010477757029248354},\n",
      "                       'face_confidence': 0.91,\n",
      "                       'gender': {'Man': 99.99967813491821,\n",
      "                                  'Woman': 0.00031815122838452226},\n",
      "                       'race': {'asian': 1.2118784710764885,\n",
      "                                'black': 0.9098192676901817,\n",
      "                                'indian': 9.848291426897049,\n",
      "                                'latino hispanic': 30.095240473747253,\n",
      "                                'middle eastern': 31.431668996810913,\n",
      "                                'white': 26.503100991249084},\n",
      "                       'region': {'h': 273,\n",
      "                                  'left_eye': (749, 249),\n",
      "                                  'right_eye': (657, 246),\n",
      "                                  'w': 273,\n",
      "                                  'x': 568,\n",
      "                                  'y': 137}}],\n",
      " 'Bernard_Arnault': [{'age': 47,\n",
      "                      'dominant_emotion': 'angry',\n",
      "                      'dominant_gender': 'Man',\n",
      "                      'dominant_race': 'white',\n",
      "                      'emotion': {'angry': 91.61304781514691,\n",
      "                                  'disgust': 0.3112636692085069,\n",
      "                                  'fear': 3.1390739613219476,\n",
      "                                  'happy': 0.006872258791536617,\n",
      "                                  'neutral': 0.4792580666695873,\n",
      "                                  'sad': 4.412448296692866,\n",
      "                                  'surprise': 0.03804005327128191},\n",
      "                      'face_confidence': 0.95,\n",
      "                      'gender': {'Man': 99.98006224632263,\n",
      "                                 'Woman': 0.019936800526920706},\n",
      "                      'race': {'asian': 0.21654064767062664,\n",
      "                               'black': 0.24555083364248276,\n",
      "                               'indian': 0.7619684096425772,\n",
      "                               'latino hispanic': 7.185497879981995,\n",
      "                               'middle eastern': 20.63450515270233,\n",
      "                               'white': 70.95593810081482},\n",
      "                      'region': {'h': 120,\n",
      "                                 'left_eye': None,\n",
      "                                 'right_eye': None,\n",
      "                                 'w': 120,\n",
      "                                 'x': 95,\n",
      "                                 'y': 67}}],\n",
      " 'Bill_Gates': [{'age': 58,\n",
      "                 'dominant_emotion': 'angry',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 82.33043762082605,\n",
      "                             'disgust': 7.626768706672167e-05,\n",
      "                             'fear': 14.133676691809095,\n",
      "                             'happy': 3.532084401597452,\n",
      "                             'neutral': 0.0022997673100370766,\n",
      "                             'sad': 0.0011788470560406118,\n",
      "                             'surprise': 0.00023580270780518503},\n",
      "                 'face_confidence': 0,\n",
      "                 'gender': {'Man': 96.17046117782593,\n",
      "                            'Woman': 3.8295403122901917},\n",
      "                 'race': {'asian': 2.271214598470251e-05,\n",
      "                          'black': 3.8117982762305974e-08,\n",
      "                          'indian': 9.932349816474884e-06,\n",
      "                          'latino hispanic': 0.03258092135224299,\n",
      "                          'middle eastern': 0.09833627854198558,\n",
      "                          'white': 99.86905454028303},\n",
      "                 'region': {'h': 473,\n",
      "                            'left_eye': None,\n",
      "                            'right_eye': None,\n",
      "                            'w': 473,\n",
      "                            'x': 0,\n",
      "                            'y': 0}}],\n",
      " 'Carlos_Slim_Helu': [{'age': 52,\n",
      "                       'dominant_emotion': 'happy',\n",
      "                       'dominant_gender': 'Man',\n",
      "                       'dominant_race': 'white',\n",
      "                       'emotion': {'angry': 7.1316441641045e-08,\n",
      "                                   'disgust': 1.4413657716653453e-18,\n",
      "                                   'fear': 3.923831259911026e-08,\n",
      "                                   'happy': 99.99891519546509,\n",
      "                                   'neutral': 0.0010832146472239401,\n",
      "                                   'sad': 2.478979044609475e-10,\n",
      "                                   'surprise': 3.088334477041599e-07},\n",
      "                       'face_confidence': 0.92,\n",
      "                       'gender': {'Man': 99.96567964553833,\n",
      "                                  'Woman': 0.034315892844460905},\n",
      "                       'race': {'asian': 0.775473564863205,\n",
      "                                'black': 0.12670623837038875,\n",
      "                                'indian': 1.0099041275680065,\n",
      "                                'latino hispanic': 10.468573123216629,\n",
      "                                'middle eastern': 22.183144092559814,\n",
      "                                'white': 65.43620228767395},\n",
      "                       'region': {'h': 129,\n",
      "                                  'left_eye': None,\n",
      "                                  'right_eye': None,\n",
      "                                  'w': 129,\n",
      "                                  'x': 145,\n",
      "                                  'y': 52}}],\n",
      " 'Elon_Musk': [{'age': 34,\n",
      "                'dominant_emotion': 'happy',\n",
      "                'dominant_gender': 'Man',\n",
      "                'dominant_race': 'asian',\n",
      "                'emotion': {'angry': 1.1242302275634703e-08,\n",
      "                            'disgust': 1.3687286494016493e-18,\n",
      "                            'fear': 4.4995373586663224e-10,\n",
      "                            'happy': 97.21547994429112,\n",
      "                            'neutral': 2.7829148283106293,\n",
      "                            'sad': 2.6528356346688293e-07,\n",
      "                            'surprise': 0.001601457543159774},\n",
      "                'face_confidence': 0.92,\n",
      "                'gender': {'Man': 99.73146319389343,\n",
      "                           'Woman': 0.26853419840335846},\n",
      "                'race': {'asian': 33.30698013305664,\n",
      "                         'black': 3.4057430922985077,\n",
      "                         'indian': 5.427328869700432,\n",
      "                         'latino hispanic': 24.275025725364685,\n",
      "                         'middle eastern': 8.83888378739357,\n",
      "                         'white': 24.746033549308777},\n",
      "                'region': {'h': 668,\n",
      "                           'left_eye': (1379, 579),\n",
      "                           'right_eye': (1149, 612),\n",
      "                           'w': 668,\n",
      "                           'x': 932,\n",
      "                           'y': 333}}],\n",
      " 'Jack_Ma': [{'age': 34,\n",
      "              'dominant_emotion': 'fear',\n",
      "              'dominant_gender': 'Man',\n",
      "              'dominant_race': 'white',\n",
      "              'emotion': {'angry': 25.811997056007385,\n",
      "                          'disgust': 5.170084080227274e-28,\n",
      "                          'fear': 54.761505126953125,\n",
      "                          'happy': 1.5395928889411215e-10,\n",
      "                          'neutral': 19.426435232162476,\n",
      "                          'sad': 6.452893330788356e-05,\n",
      "                          'surprise': 8.328142088328813e-16},\n",
      "              'face_confidence': 0,\n",
      "              'gender': {'Man': 87.47439980506897, 'Woman': 12.52560168504715},\n",
      "              'race': {'asian': 1.3496225699782372,\n",
      "                       'black': 0.337715819478035,\n",
      "                       'indian': 0.7685142103582621,\n",
      "                       'latino hispanic': 12.235289067029953,\n",
      "                       'middle eastern': 9.776079654693604,\n",
      "                       'white': 75.53277611732483},\n",
      "              'region': {'h': 233,\n",
      "                         'left_eye': None,\n",
      "                         'right_eye': None,\n",
      "                         'w': 391,\n",
      "                         'x': 0,\n",
      "                         'y': 0}}],\n",
      " 'Jeff_Bezos': [{'age': 37,\n",
      "                 'dominant_emotion': 'sad',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 28.301414847373962,\n",
      "                             'disgust': 0.20275006536394358,\n",
      "                             'fear': 5.60418963432312,\n",
      "                             'happy': 0.09996638400480151,\n",
      "                             'neutral': 30.220091342926025,\n",
      "                             'sad': 34.835854172706604,\n",
      "                             'surprise': 0.7357343100011349},\n",
      "                 'face_confidence': 0.94,\n",
      "                 'gender': {'Man': 99.9863862991333,\n",
      "                            'Woman': 0.013613494229502976},\n",
      "                 'race': {'asian': 5.187125504016876,\n",
      "                          'black': 2.921084500849247,\n",
      "                          'indian': 5.6783705949783325,\n",
      "                          'latino hispanic': 20.033560693264008,\n",
      "                          'middle eastern': 27.553242444992065,\n",
      "                          'white': 38.626620173454285},\n",
      "                 'region': {'h': 91,\n",
      "                            'left_eye': None,\n",
      "                            'right_eye': None,\n",
      "                            'w': 91,\n",
      "                            'x': 144,\n",
      "                            'y': 24}}],\n",
      " 'Larry_Ellison': [{'age': 41,\n",
      "                    'dominant_emotion': 'angry',\n",
      "                    'dominant_gender': 'Man',\n",
      "                    'dominant_race': 'asian',\n",
      "                    'emotion': {'angry': 37.24592924118042,\n",
      "                                'disgust': 5.683257058262825,\n",
      "                                'fear': 17.678695917129517,\n",
      "                                'happy': 0.2646475564688444,\n",
      "                                'neutral': 14.047569036483765,\n",
      "                                'sad': 24.50806051492691,\n",
      "                                'surprise': 0.5718388594686985},\n",
      "                    'face_confidence': 0.93,\n",
      "                    'gender': {'Man': 99.04099702835083,\n",
      "                               'Woman': 0.9589989669620991},\n",
      "                    'race': {'asian': 40.73759317398071,\n",
      "                             'black': 10.534483939409256,\n",
      "                             'indian': 15.150125324726105,\n",
      "                             'latino hispanic': 20.574641227722168,\n",
      "                             'middle eastern': 5.419602990150452,\n",
      "                             'white': 7.583555579185486},\n",
      "                    'region': {'h': 64,\n",
      "                               'left_eye': None,\n",
      "                               'right_eye': None,\n",
      "                               'w': 64,\n",
      "                               'x': 163,\n",
      "                               'y': 31}}],\n",
      " 'Larry_Page': [{'age': 24,\n",
      "                 'dominant_emotion': 'neutral',\n",
      "                 'dominant_gender': 'Man',\n",
      "                 'dominant_race': 'white',\n",
      "                 'emotion': {'angry': 3.8204073905944824,\n",
      "                             'disgust': 0.0012996045370528009,\n",
      "                             'fear': 0.05951942293904722,\n",
      "                             'happy': 1.7419662326574326,\n",
      "                             'neutral': 90.8681333065033,\n",
      "                             'sad': 3.1853381544351578,\n",
      "                             'surprise': 0.3233378520235419},\n",
      "                 'face_confidence': 0.92,\n",
      "                 'gender': {'Man': 99.91961121559143,\n",
      "                            'Woman': 0.0803928473033011},\n",
      "                 'race': {'asian': 1.3089824770113854,\n",
      "                          'black': 0.4460215968165878,\n",
      "                          'indian': 3.3429961391691996,\n",
      "                          'latino hispanic': 18.484360756354242,\n",
      "                          'middle eastern': 25.147539359624464,\n",
      "                          'white': 51.2700913822542},\n",
      "                 'region': {'h': 96,\n",
      "                            'left_eye': (202, 89),\n",
      "                            'right_eye': (168, 91),\n",
      "                            'w': 96,\n",
      "                            'x': 139,\n",
      "                            'y': 52}}],\n",
      " 'Mark_Zukerberg': [{'age': 34,\n",
      "                     'dominant_emotion': 'happy',\n",
      "                     'dominant_gender': 'Woman',\n",
      "                     'dominant_race': 'white',\n",
      "                     'emotion': {'angry': 1.402378494770205e-12,\n",
      "                                 'disgust': 4.4871832514165026e-27,\n",
      "                                 'fear': 9.155249908987936e-16,\n",
      "                                 'happy': 100.0,\n",
      "                                 'neutral': 3.189859043573051e-06,\n",
      "                                 'sad': 1.443973114906641e-11,\n",
      "                                 'surprise': 1.3284874589292173e-08},\n",
      "                     'face_confidence': 0.94,\n",
      "                     'gender': {'Man': 26.580575108528137,\n",
      "                                'Woman': 73.41942191123962},\n",
      "                     'race': {'asian': 0.24163080379366875,\n",
      "                              'black': 0.025132347946055233,\n",
      "                              'indian': 0.4272217396646738,\n",
      "                              'latino hispanic': 15.532532334327698,\n",
      "                              'middle eastern': 18.303953111171722,\n",
      "                              'white': 65.46952724456787},\n",
      "                     'region': {'h': 113,\n",
      "                                'left_eye': (229, 84),\n",
      "                                'right_eye': (194, 80),\n",
      "                                'w': 113,\n",
      "                                'x': 154,\n",
      "                                'y': 38}}],\n",
      " 'Warren_Buffett': [{'age': 44,\n",
      "                     'dominant_emotion': 'happy',\n",
      "                     'dominant_gender': 'Man',\n",
      "                     'dominant_race': 'white',\n",
      "                     'emotion': {'angry': 0.0006734751423209673,\n",
      "                                 'disgust': 6.875067853749249e-13,\n",
      "                                 'fear': 0.027919866261072457,\n",
      "                                 'happy': 95.83462476730347,\n",
      "                                 'neutral': 4.135195165872574,\n",
      "                                 'sad': 0.0011482700756459963,\n",
      "                                 'surprise': 0.00043960112634522375},\n",
      "                     'face_confidence': 0.93,\n",
      "                     'gender': {'Man': 99.86355900764465,\n",
      "                                'Woman': 0.13643514830619097},\n",
      "                     'race': {'asian': 0.5063895929438361,\n",
      "                              'black': 0.049324601363810734,\n",
      "                              'indian': 0.23208439377434328,\n",
      "                              'latino hispanic': 7.3870761827139635,\n",
      "                              'middle eastern': 5.6063760839252375,\n",
      "                              'white': 86.21874615922616},\n",
      "                     'region': {'h': 163,\n",
      "                                'left_eye': None,\n",
      "                                'right_eye': None,\n",
      "                                'w': 163,\n",
      "                                'x': 144,\n",
      "                                'y': 13}}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "final_result = {}\n",
    "\n",
    "for filename in os.listdir(\".\\images\\individuals\"):\n",
    "    if filename.endswith(\".jpeg\"):\n",
    "        known_image = face_recognition.load_image_file(f\".\\Images\\Individuals\\{filename}\")\n",
    "        known_image = cv2.cvtColor(known_image, cv2.COLOR_BGR2RGB)\n",
    "        try:\n",
    "            detector = DeepFace.analyze(known_image, enforce_detection=False)\n",
    "            final_result[filename.replace(\".jpeg\", \"\")] = detector\n",
    "        except ValueError as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "pprint.pprint(final_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Face Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Loop over each frame from the video until interrupted (e.g., by pressing 'q')\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame from BGR (OpenCV) to RGB (face_recognition)\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Find all face locations and encodings in the frame\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Loop over each face in the frame\n",
    "    for face_encoding, face_location in zip(face_encodings, face_locations):\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "\n",
    "        # Check if there is a match and get the corresponding name\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        top, right, bottom, left = face_location\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Label the face with the name\n",
    "        cv2.putText(frame, name, (left + 6, bottom - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Facial Attribute Detection (Emotion Recognition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from fer import FER\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize FER emotion detector\n",
    "emotion_detector = FER()\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Check if frame is successfully captured\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Detect emotions in the frame\n",
    "    emotions = emotion_detector.detect_emotions(frame)\n",
    "\n",
    "    # Loop through detected faces and emotions\n",
    "    for emotion_data in emotions:\n",
    "        # Get bounding box (face location) and emotions for each face\n",
    "        (top, right, bottom, left) = emotion_data['box']\n",
    "        emotion, score = max(emotion_data['emotions'].items(), key=lambda item: item[1])\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Display the emotion and score near the face\n",
    "        text = f\"{emotion}: {score:.2f}\"\n",
    "        cv2.putText(frame, text, (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 255, 255), 2)\n",
    "\n",
    "    # Display the resulting frame\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real-Time Facial Landmarks Detection (DeepFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race:  75%|███████▌  | 3/4 [00:05<00:01,  1.90s/it]   \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Analyze the face with DeepFace to detect age, gender, emotion\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 31\u001b[0m     analysis \u001b[38;5;241m=\u001b[39m \u001b[43mDeepFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mface_image\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m     age \u001b[38;5;241m=\u001b[39m analysis[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     33\u001b[0m     gender \u001b[38;5;241m=\u001b[39m analysis[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdominant_gender\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\deepface\\DeepFace.py:253\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze\u001b[39m(\n\u001b[0;32m    167\u001b[0m     img_path: Union[\u001b[38;5;28mstr\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray],\n\u001b[0;32m    168\u001b[0m     actions: Union[\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m] \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124memotion\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgender\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    174\u001b[0m     anti_spoofing: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    175\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Dict[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    176\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;124;03m    Analyze facial attributes such as age, gender, emotion, and race in the provided image.\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;124;03m            - 'white': Confidence score for White ethnicity.\u001b[39;00m\n\u001b[0;32m    252\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdemography\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_detection\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_detection\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdetector_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdetector_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43malign\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malign\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexpand_percentage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpand_percentage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msilent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43manti_spoofing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manti_spoofing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\deepface\\modules\\demography.py:192\u001b[0m, in \u001b[0;36manalyze\u001b[1;34m(img_path, actions, enforce_detection, detector_backend, align, expand_percentage, silent, anti_spoofing)\u001b[0m\n\u001b[0;32m    189\u001b[0m     obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdominant_gender\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m Gender\u001b[38;5;241m.\u001b[39mlabels[np\u001b[38;5;241m.\u001b[39margmax(gender_predictions)]\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 192\u001b[0m     race_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mmodeling\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfacial_attribute\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRace\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[0;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpredict(img_content)\n\u001b[0;32m    195\u001b[0m     sum_of_predictions \u001b[38;5;241m=\u001b[39m race_predictions\u001b[38;5;241m.\u001b[39msum()\n\u001b[0;32m    197\u001b[0m     obj[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrace\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\deepface\\modules\\modeling.py:96\u001b[0m, in \u001b[0;36mbuild_model\u001b[1;34m(task, model_name)\u001b[0m\n\u001b[0;32m     94\u001b[0m model \u001b[38;5;241m=\u001b[39m models[task]\u001b[38;5;241m.\u001b[39mget(model_name)\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m---> 96\u001b[0m     cached_models[task][model_name] \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid model_name passed - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtask\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\deepface\\models\\demography\\Race.py:39\u001b[0m, in \u001b[0;36mRaceClient.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRace\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\deepface\\models\\demography\\Race.py:80\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(url)\u001b[0m\n\u001b[0;32m     77\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(output)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m will be downloaded...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     78\u001b[0m     gdown\u001b[38;5;241m.\u001b[39mdownload(url, output, quiet\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m---> 80\u001b[0m \u001b[43mrace_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m race_model\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\engine\\training.py:3230\u001b[0m, in \u001b[0;36mModel.load_weights\u001b[1;34m(self, filepath, skip_mismatch, by_name, options)\u001b[0m\n\u001b[0;32m   3178\u001b[0m \u001b[38;5;129m@traceback_utils\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_traceback\n\u001b[0;32m   3179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_weights\u001b[39m(\n\u001b[0;32m   3180\u001b[0m     \u001b[38;5;28mself\u001b[39m, filepath, skip_mismatch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, by_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3181\u001b[0m ):\n\u001b[0;32m   3182\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Loads all layer weights from a saved files.\u001b[39;00m\n\u001b[0;32m   3183\u001b[0m \n\u001b[0;32m   3184\u001b[0m \u001b[38;5;124;03m    The saved file could be a SavedModel file, a `.keras` file (v3 saving\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3228\u001b[0m \u001b[38;5;124;03m            options for loading weights (only valid for a SavedModel file).\u001b[39;00m\n\u001b[0;32m   3229\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 3230\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msaving_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3231\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3232\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3233\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3234\u001b[0m \u001b[43m        \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_mismatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3235\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3236\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\saving\\saving_api.py:305\u001b[0m, in \u001b[0;36mload_weights\u001b[1;34m(model, filepath, skip_mismatch, **kwargs)\u001b[0m\n\u001b[0;32m    301\u001b[0m     saving_lib\u001b[38;5;241m.\u001b[39mload_weights_only(\n\u001b[0;32m    302\u001b[0m         model, filepath, skip_mismatch\u001b[38;5;241m=\u001b[39mskip_mismatch\n\u001b[0;32m    303\u001b[0m     )\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 305\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlegacy_sm_saving_lib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    306\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_mismatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_mismatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\save.py:493\u001b[0m, in \u001b[0;36mload_weights\u001b[1;34m(model, filepath, by_name, skip_mismatch, options)\u001b[0m\n\u001b[0;32m    489\u001b[0m             hdf5_format\u001b[38;5;241m.\u001b[39mload_weights_from_hdf5_group_by_name(\n\u001b[0;32m    490\u001b[0m                 f, model, skip_mismatch\n\u001b[0;32m    491\u001b[0m             )\n\u001b[0;32m    492\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 493\u001b[0m             \u001b[43mhdf5_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;66;03m# Perform any layer defined finalization of the layer state.\u001b[39;00m\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m model\u001b[38;5;241m.\u001b[39mlayers:\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\hdf5_format.py:832\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    830\u001b[0m layer \u001b[38;5;241m=\u001b[39m filtered_layers[k]\n\u001b[0;32m    831\u001b[0m symbolic_weights \u001b[38;5;241m=\u001b[39m _legacy_weights(layer)\n\u001b[1;32m--> 832\u001b[0m weight_values \u001b[38;5;241m=\u001b[39m \u001b[43mload_subset_weights_from_hdf5_group\u001b[49m\u001b[43m(\u001b[49m\u001b[43mg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m weight_values \u001b[38;5;241m=\u001b[39m preprocess_weights_for_loading(\n\u001b[0;32m    834\u001b[0m     layer, weight_values, original_keras_version, original_backend\n\u001b[0;32m    835\u001b[0m )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(weight_values) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(symbolic_weights):\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\hdf5_format.py:777\u001b[0m, in \u001b[0;36mload_subset_weights_from_hdf5_group\u001b[1;34m(f)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load layer weights of a model from hdf5.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m        and weights file.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    776\u001b[0m weight_names \u001b[38;5;241m=\u001b[39m load_attributes_from_hdf5_group(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_names\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweight_name\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mweight_names\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\tf_keras\\src\\saving\\legacy\\hdf5_format.py:777\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    764\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load layer weights of a model from hdf5.\u001b[39;00m\n\u001b[0;32m    765\u001b[0m \n\u001b[0;32m    766\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    774\u001b[0m \u001b[38;5;124;03m        and weights file.\u001b[39;00m\n\u001b[0;32m    775\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    776\u001b[0m weight_names \u001b[38;5;241m=\u001b[39m load_attributes_from_hdf5_group(f, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_names\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 777\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [np\u001b[38;5;241m.\u001b[39masarray(f[weight_name]) \u001b[38;5;28;01mfor\u001b[39;00m weight_name \u001b[38;5;129;01min\u001b[39;00m weight_names]\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mh5py\\\\_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\h5py\\_hl\\dataset.py:1063\u001b[0m, in \u001b[0;36mDataset.__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m   1061\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\n\u001b[1;32m-> 1063\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_direct\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "File \u001b[1;32md:\\Projects\\AI Tutorials\\TensorFlow\\Face Recognition\\venv\\Lib\\site-packages\\h5py\\_hl\\dataset.py:1024\u001b[0m, in \u001b[0;36mDataset.read_direct\u001b[1;34m(self, dest, source_sel, dest_sel)\u001b[0m\n\u001b[0;32m   1021\u001b[0m     dest_sel \u001b[38;5;241m=\u001b[39m sel\u001b[38;5;241m.\u001b[39mselect(dest\u001b[38;5;241m.\u001b[39mshape, dest_sel)\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m mspace \u001b[38;5;129;01min\u001b[39;00m dest_sel\u001b[38;5;241m.\u001b[39mbroadcast(source_sel\u001b[38;5;241m.\u001b[39marray_shape):\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "\n",
    "# Initialize Dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')  \n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "\n",
    "    # Detect faces using Dlib\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    faces = detector(gray)\n",
    "\n",
    "    # Analyze attributes using DeepFace for each detected face\n",
    "    for face in faces:\n",
    "        # Get bounding box coordinates for the face\n",
    "        x, y, w, h = (face.left(), face.top(), face.width(), face.height())\n",
    "\n",
    "        # Extract the face region from the frame and convert it to an image\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Analyze the face with DeepFace to detect age, gender, emotion\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(face_image, enforce_detection=False)\n",
    "            age = analysis[0]['age']\n",
    "            gender = analysis[0]['dominant_gender']\n",
    "            emotion = analysis[0]['dominant_emotion']\n",
    "            # race = analysis[0]['dominant_race']\n",
    "            \n",
    "            # Display the detected age, gender, and emotion\n",
    "            cv2.putText(frame, f'Age: {age}', (x, y - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (x, y + h + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error analyzing face: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Detect and display facial landmarks using Dlib\n",
    "        landmarks = predictor(gray, face)\n",
    "        for n in range(0, 68):  # Loop over the 68 facial landmarks\n",
    "            x_point = landmarks.part(n).x\n",
    "            y_point = landmarks.part(n).y\n",
    "            cv2.circle(frame, (x_point, y_point), 2, (0, 255, 0), -1)\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Beast MODE\n",
    "#### (Real-Time Face Recognition with facial landmarks,age,gender and emotions Predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Action: race: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.96it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.81it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.15it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:00<00:00,  4.07it/s]  \n",
      "Action: race: 100%|██████████| 4/4 [00:01<00:00,  3.97it/s]  \n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "from deepface import DeepFace\n",
    "import face_recognition\n",
    "\n",
    "# Initialize Dlib's face detector and facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# Initialize the webcam\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = video_capture.read()\n",
    "    if not ret:\n",
    "        print(\"Failed to grab frame\")\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale for Dlib\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Convert the frame from BGR to RGB for face_recognition\n",
    "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Detect faces using Dlib\n",
    "    dlib_faces = detector(gray)\n",
    "\n",
    "    # Detect faces using face_recognition\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    face_encodings = face_recognition.face_encodings(rgb_frame, face_locations)\n",
    "\n",
    "    # Process each detected face\n",
    "    for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "        # Face recognition\n",
    "        matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "        name = \"Unknown\"\n",
    "        if True in matches:\n",
    "            first_match_index = matches.index(True)\n",
    "            name = known_face_names[first_match_index]\n",
    "\n",
    "        # Extract the face region\n",
    "        face_image = frame[top:bottom, left:right]\n",
    "        face_image_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Analyze the face with DeepFace\n",
    "        try:\n",
    "            analysis = DeepFace.analyze(face_image_rgb, enforce_detection=False)\n",
    "            age = analysis[0]['age']\n",
    "            gender = analysis[0]['dominant_gender']\n",
    "            emotion = analysis[0]['dominant_emotion']\n",
    "\n",
    "            # Display results\n",
    "            cv2.putText(frame, f'Name: {name}', (left, top - 50), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Age: {age}', (left, top - 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Gender: {gender}', (left, top - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "            cv2.putText(frame, f'Emotion: {emotion}', (left, bottom + 20), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        except ValueError as e:\n",
    "            print(f\"Error analyzing face: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "        # Detect and display facial landmarks using Dlib\n",
    "        dlib_face = dlib.rectangle(left, top, right, bottom)\n",
    "        landmarks = predictor(gray, dlib_face)\n",
    "        for n in range(0, 68):  # Loop over the 68 facial landmarks\n",
    "            x_point = landmarks.part(n).x\n",
    "            y_point = landmarks.part(n).y\n",
    "            cv2.circle(frame, (x_point, y_point), 2, (0, 255, 0), -1)\n",
    "\n",
    "    # Display the video feed\n",
    "    cv2.imshow('Video', frame)\n",
    "\n",
    "    # Press 'q' to quit\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close the window\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE can make this a functional Function in a ELearning platform to track student learning.\n",
    "- based on the studednt profile bicture. and name\n",
    "- generate a report about the student learning session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Work & Potential Integrations\n",
    "\n",
    "1. **Attention Mechanism**:\n",
    "   - You could implement an attention mechanism in your face recognition system that allows the model to focus on specific parts of the input image or sequence of frames. This could enhance the accuracy of facial attribute detection and recognition.\n",
    "   - Attention can help in interpreting which areas of the face are most relevant when making predictions, possibly improving the results of your facial analysis.\n",
    "\n",
    "2. **Image Captioning**:\n",
    "   - Use NLP to generate descriptive captions for images detected in the video feed. For instance, once a face is recognized and analyzed, you could create a textual summary of the analysis results (e.g., \"This is John, a 25-year-old male, looking happy.\").\n",
    "   - You could combine visual features extracted from the image with textual descriptions for enhanced data representation.\n",
    "\n",
    "3. **Sentiment Analysis**:\n",
    "   - Analyze the emotions detected using DeepFace and combine this with NLP techniques to evaluate sentiment in any text associated with the detected individual. For example, if a recognized person has spoken text (like a transcript), you could analyze how their spoken words relate to their facial expressions.\n",
    "   - This could be particularly useful in applications like customer service or interactive AI where understanding both visual and textual cues is important.\n",
    "\n",
    "4. **Using Grad-CAM for Interpretability**:\n",
    "   - Grad-CAM (Gradient-weighted Class Activation Mapping) can be utilized to visualize which parts of the image influenced the model’s predictions. After facial analysis, you can overlay Grad-CAM heatmaps on the detected faces to show which facial features led to specific attributes or recognition results.\n",
    "   - This can improve the interpretability of your model, allowing users to understand why certain predictions were made.\n",
    "\n",
    "5. **Multimodal Data Analysis**:\n",
    "   - Combine text data (e.g., transcripts, comments) with visual data (video or images) for a more holistic analysis. For example, when analyzing a video, you can detect faces and simultaneously transcribe speech, analyzing how facial expressions align with spoken content.\n",
    "\n",
    "### Example Workflow\n",
    "\n",
    "1. **Face Detection and Analysis**:\n",
    "   - Use the existing face recognition system to detect and analyze faces in real-time.\n",
    "\n",
    "2. **Text Data Integration**:\n",
    "   - As faces are recognized, pull any associated textual data (like comments or transcripts) and analyze it using NLP techniques.\n",
    "\n",
    "3. **Generate Captions**:\n",
    "   - Use NLP to generate captions or textual descriptions based on the results of facial analysis (age, gender, emotions).\n",
    "\n",
    "4. **Visualize Attention with Grad-CAM**:\n",
    "   - Apply Grad-CAM to visualize which facial regions contributed to the attribute predictions, displaying these overlays on the video feed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
